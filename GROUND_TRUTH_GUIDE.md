# Ground Truth Implementation Guide

## What is Ground Truth?

**Ground Truth** is the "correct answer" or "gold standard" data that we use to evaluate how well a PDF parser performs. It's manually verified, accurate data that represents what a perfect parser should extract from each document.

## Current Problem: Mock Ground Truth

The current `.fire.json` files are **NOT real ground truth** - they're generated by the same mock parser we're testing! This creates a circular validation problem:

```
Mock Parser → Generates "Ground Truth" → Tests Against Same Mock Parser → Always Passes ✓
```

## Real Ground Truth Implementation

### 1. Manual Annotation Process

**Step 1: Human Review**
```bash
# For each PDF, a human annotator would:
1. Open the PDF manually
2. Identify actual document sections
3. Note real page ranges
4. Extract actual fire piping entities
5. Create accurate .fire.json file
```

**Example: Real vs Mock Ground Truth**

**Mock Ground Truth (Current):**
```json
{
  "chunks": [
    {"title": "FIRE PROTECTION SYSTEM OVERVIEW", "start_page": 1, "end_page": 3},
    {"title": "MATERIAL SPECIFICATIONS", "start_page": 3, "end_page": 5},
    {"title": "INSTALLATION REQUIREMENTS", "start_page": 7, "end_page": 9}
  ]
}
```

**Real Ground Truth (Manual Review of NYC_HPD_Table_of_Contents.pdf):**
```json
{
  "chunks": [
    {"title": "Table of Contents", "start_page": 1, "end_page": 1},
    {"title": "Division 21 - Fire Suppression", "start_page": 2, "end_page": 2},
    {"title": "21 05 00 - Common Work Results for Fire Suppression", "start_page": 3, "end_page": 3},
    {"title": "21 13 13 - Wet-Pipe Sprinkler Systems", "start_page": 4, "end_page": 4},
    {"title": "21 13 16 - Dry-Pipe Sprinkler Systems", "start_page": 5, "end_page": 5}
  ],
  "entities": [
    {"id": "spec_001", "type": "specification", "section": "21 05 00", "title": "Common Work Results", "page": 3},
    {"id": "spec_002", "type": "specification", "section": "21 13 13", "title": "Wet-Pipe Systems", "page": 4}
  ]
}
```

### 2. Ground Truth Creation Tools

**Interactive Annotation Tool:**
```python
# Enhanced ground_truth_generator.py with manual review
def create_ground_truth_interactive(pdf_path: Path) -> Dict:
    """Interactive ground truth creation with human validation."""
    
    # 1. Display PDF pages to human annotator
    display_pdf_pages(pdf_path)
    
    # 2. Collect manual annotations
    chunks = []
    print(f"Annotating: {pdf_path.name}")
    
    while True:
        title = input("Section title (or 'done'): ")
        if title.lower() == 'done':
            break
            
        start_page = int(input("Start page: "))
        end_page = int(input("End page: "))
        
        chunks.append({
            "title": title,
            "start_page": start_page,
            "end_page": end_page,
            "confidence": "manual_annotation",
            "annotator": get_annotator_id()
        })
    
    # 3. Collect entity annotations
    entities = annotate_entities_interactive(pdf_path)
    
    return {
        "metadata": {
            "pdf_name": pdf_path.name,
            "annotation_method": "manual",
            "annotator": get_annotator_id(),
            "annotation_date": datetime.now().isoformat(),
            "quality_score": 1.0  # Manual annotation is highest quality
        },
        "chunks": chunks,
        "entities": entities
    }
```

**Semi-Automated Approach:**
```python
def create_ground_truth_assisted(pdf_path: Path) -> Dict:
    """Use multiple parsers + human verification."""
    
    # 1. Run multiple parsers
    results = []
    for parser in [pdfplumber_parser, pymupdf_parser, pdfminer_parser]:
        try:
            result = parser.parse(pdf_path)
            results.append(result)
        except Exception as e:
            print(f"Parser {parser.name} failed: {e}")
    
    # 2. Merge and present to human for verification
    merged_chunks = merge_parser_results(results)
    verified_chunks = human_verify_chunks(merged_chunks, pdf_path)
    
    return create_ground_truth_from_verified(verified_chunks)
```

### 3. Quality Control Process

**Multi-Annotator Agreement:**
```python
def validate_ground_truth_quality(annotations: List[Dict]) -> float:
    """Calculate inter-annotator agreement score."""
    
    # Cohen's Kappa or similar metric for chunk boundaries
    # Exact match percentage for entity extraction
    agreement_score = calculate_inter_annotator_agreement(annotations)
    
    if agreement_score < 0.8:
        print("⚠️  Low annotator agreement - needs review")
        
    return agreement_score
```

**Validation Checks:**
```python
def validate_ground_truth_integrity(gt_data: Dict) -> List[str]:
    """Validate ground truth data integrity."""
    issues = []
    
    # Check chunk continuity
    chunks = gt_data["chunks"]
    for i in range(len(chunks) - 1):
        if chunks[i]["end_page"] >= chunks[i+1]["start_page"]:
            issues.append(f"Overlapping chunks: {chunks[i]['title']} and {chunks[i+1]['title']}")
    
    # Check entity page references
    for entity in gt_data["entities"]:
        if not any(chunk["start_page"] <= entity["location_page"] <= chunk["end_page"] 
                  for chunk in chunks):
            issues.append(f"Entity {entity['id']} on unreferenced page {entity['location_page']}")
    
    return issues
```

### 4. Ground Truth Management System

**Version Control:**
```bash
# Store ground truth in version control
ground_truth/
├── v1.0/
│   ├── NYC_HPD_Table_of_Contents.fire.json
│   └── metadata.json
├── v1.1/  # Updated annotations
│   ├── NYC_HPD_Table_of_Contents.fire.json
│   └── metadata.json
└── validation_reports/
    ├── inter_annotator_agreement.json
    └── quality_metrics.json
```

**Ground Truth Database:**
```python
class GroundTruthDB:
    """Manage ground truth versions and quality metrics."""
    
    def store_annotation(self, pdf_name: str, annotation: Dict, version: str):
        """Store validated ground truth annotation."""
        
    def get_latest_ground_truth(self, pdf_name: str) -> Dict:
        """Get highest quality ground truth for PDF."""
        
    def compare_annotations(self, pdf_name: str, versions: List[str]) -> Dict:
        """Compare different annotation versions."""
        
    def get_quality_metrics(self, pdf_name: str) -> Dict:
        """Get annotation quality and agreement scores."""
```

## Implementation Steps

### Phase 1: Manual Ground Truth Creation (High Quality, Small Scale)
1. **Select 10-20 representative PDFs** from different document types
2. **Manual annotation** by domain experts (fire protection engineers)
3. **Create reference standard** with 95%+ accuracy
4. **Use for parser development** and initial validation

### Phase 2: Semi-Automated Ground Truth (Medium Quality, Medium Scale)
1. **Run multiple best-in-class parsers** on remaining PDFs
2. **Consensus-based ground truth** from parser agreement
3. **Spot-check validation** by humans on sample
4. **Use for regression testing** and performance monitoring

### Phase 3: Automated Ground Truth (Lower Quality, Large Scale)
1. **Use best performing parser** from Phase 1/2 validation
2. **Generate ground truth** for new PDFs automatically
3. **Statistical validation** and outlier detection
4. **Use for smoke testing** and continuous integration

## Updated Ground Truth Generator

Here's how to implement real ground truth creation:

```python
# Enhanced tests/ground_truth_generator.py
class RealGroundTruthGenerator:
    
    def create_manual_ground_truth(self, pdf_path: Path) -> Path:
        """Create ground truth through manual annotation."""
        
        # 1. Extract text and display to annotator
        pdf_text = extract_pdf_text_with_pages(pdf_path)
        
        # 2. Interactive annotation session
        chunks = self.annotate_chunks_manual(pdf_text, pdf_path)
        entities = self.annotate_entities_manual(pdf_text, pdf_path)
        
        # 3. Quality validation
        issues = validate_annotation_quality(chunks, entities)
        if issues:
            print("Quality issues found:", issues)
            if not confirm_continue():
                return None
        
        # 4. Save with metadata
        ground_truth = {
            "metadata": {
                "creation_method": "manual_annotation",
                "annotator": input("Annotator name: "),
                "annotation_date": datetime.now().isoformat(),
                "pdf_pages": get_pdf_page_count(pdf_path),
                "quality_level": "gold_standard"
            },
            "chunks": chunks,
            "entities": entities
        }
        
        output_path = pdf_path.with_suffix('.ground_truth.json')
        save_ground_truth(ground_truth, output_path)
        return output_path
```

The key insight: **Real ground truth requires human expertise** - especially for fire protection documents where domain knowledge is critical for accurate annotation. 